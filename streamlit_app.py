# streamlit_app.py
# ------------------------------------------------------------
# ODISSE ‚Äî visualisations multi-sources (APIs ODISSE + CSV + CSV pr√©diction)
# ------------------------------------------------------------
import io
import os
import time
import requests
import pandas as pd
import streamlit as st

# Visus (optionnels si non dispo)
try:
    import plotly.express as px
except Exception:
    px = None  # on g√©rera plus bas

st.set_page_config(page_title="ODISSE ‚Äî Multi-cartes & Pr√©dictions", layout="wide")


# ===== 1) DATASETS ============================================================
# kind: "api" ou "csv"
# is_prediction: True pour activer l'UI pr√©diction (KPI, badges, graph d√©di√©)
DATASETS = [
    # --- APIs ODISSE (limit√©es √† 100) ---
    {
        "label": "Couvertures vaccinales ‚Äî ados & adultes (d√©partement)",
        "kind": "api",
        "path": (
            "https://odisse.santepubliquefrance.fr/api/explore/v2.1/catalog/datasets/"
            "couvertures-vaccinales-des-adolescent-et-adultes-departement/records?limit=100"
        ),
        "value_pref": ["couverture", "taux", "pct", "pourcent", "value"],
    },
    {
        "label": "Grippe ‚Äî urgences & SOS M√©decins (d√©partement)",
        "kind": "api",
        "path": (
            "https://odisse.santepubliquefrance.fr/api/explore/v2.1/catalog/datasets/"
            "grippe-passages-aux-urgences-et-actes-sos-medecins-departement/records?limit=100"
        ),
        "value_pref": ["passages", "actes", "nb", "taux", "incidence", "value"],
    },
    {
        "label": "Grippe ‚Äî urgences & SOS M√©decins (France)",
        "kind": "api",
        "path": (
            "https://odisse.santepubliquefrance.fr/api/explore/v2.1/catalog/datasets/"
            "grippe-passages-aux-urgences-et-actes-sos-medecins-france/records?limit=100"
        ),
        # dataset national ‚Üí pas de carte si pas de colonne d√©partement
        "value_pref": ["passages", "actes", "nb", "taux", "incidence", "value"],
    },

    # --- CSV locaux (mets-les dans ./data/) ---
    {
        "label": "Couvertures locales 2024 (d√©partement)",
        "kind": "csv",
        "path": "data/couverture-2024.csv",
        "value_pref": ["couverture", "taux", "pct", "pourcent", "value"],
    },
    {
        "label": "Campagne 2024 (d√©partement)",
        "kind": "csv",
        "path": "data/campagne-2024.csv",
        "value_pref": ["passages", "actes", "nb", "taux", "incidence", "value"],
    },
    {
        "label": "Doses & actes 2024 (d√©partement)",
        "kind": "csv",
        "path": "data/doses-actes-2024.csv",
        "value_pref": ["doses", "actes", "nb", "taux", "incidence", "value"],
    },

    # --- CSV PR√âDICTION (ann√©e √† venir) ---
    # Adapte le nom de fichier ci-dessous √† ton CSV r√©el dans ./data/
    {
        "label": "Pr√©diction vaccination ‚Äî ann√©e √† venir",
        "kind": "csv",
        "path": "data/prediction-vaccination-annee-prochaine.csv",
        "is_prediction": True,
        # colonnes que tu m'as d√©crites : Serie, ann√©e, y_pred, population,
        # dose par sch√©ma (pas essentiel), Doses totales arrondies,
        # Doses avec marges arrondies
        "value_pref": ["y_pred", "taux", "couverture"],  # pour graph lignes/barres
    },
    {
        "label": "Pr√©diction risque grippe 2025 (r√©gions)",
        "kind": "csv",
        "path": "data/predictions_risque_grippe_2025.csv",
        "is_prediction": True,
        "is_risk_map": True,  # Active le rendu de carte de risque
        "value_pref": ["Score_Risque_Predit_2025"],
    },
]
# ============================================================================

# ===== 2) Utilitaires communs ===============================================
DEP_CANDIDATES = [
    "dep", "code_dep", "departement", "code_departement",
    "code_dpt", "departement_code", "CODDEP", "DEP", "code"
]
YEAR_CANDIDATES = ["annee", "year", "Annee", "ANNEE"]
DATE_CANDIDATES = ["date", "Date", "DATE"]

# Dictionnaire de d√©finitions m√©dicales/techniques
DEFINITIONS = {
    "hpv1": "HPV1 : Premi√®re dose du vaccin contre le papillomavirus humain (protection contre cancers et verrues g√©nitales)",
    "hpv2": "HPV2 : Deuxi√®me dose du vaccin HPV (compl√®te le sch√©ma vaccinal 11-14 ans)",
    "hpv3": "HPV3 : Troisi√®me dose du vaccin HPV (sch√©ma de rattrapage 15-19 ans)",
    "dtpolio": "dTPolio : Vaccin dipht√©rie-t√©tanos-poliomy√©lite (rappel adulte)",
    "grippe": "Grippe : Vaccination contre la grippe saisonni√®re",
    "covid": "COVID-19 : Vaccination contre le coronavirus SARS-CoV-2",
    "meningo": "M√©ningocoque : Vaccination contre les m√©ningites bact√©riennes",
    "passages": "Passages aux urgences : Nombre de consultations aux services d'urgences",
    "actes": "Actes SOS M√©decins : Nombre d'interventions de SOS M√©decins",
    "incidence": "Incidence : Nombre de nouveaux cas pour 100 000 habitants",
    "couverture": "Couverture vaccinale : Pourcentage de la population vaccin√©e",
    "score_risque": "Score de risque : Indicateur de risque grippe (< 25 = faible, 25-50 = moyen, > 50 = √©lev√©)",
    "risque": "Risque grippe : Probabilit√© d'augmentation des cas par rapport √† l'ann√©e pr√©c√©dente",
}

def get_legend_for_data(df: pd.DataFrame) -> list[str]:
    """D√©tecte les termes m√©dicaux pr√©sents dans les donn√©es et retourne leurs d√©finitions."""
    legends = []
    cols_lower = [c.lower() for c in df.columns]
    
    for term, definition in DEFINITIONS.items():
        if any(term in col for col in cols_lower):
            legends.append(definition)
    
    return legends

def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """Nettoie et normalise les noms de colonnes pour un affichage coh√©rent."""
    rename_map = {}
    for col in df.columns:
        new_name = col
        # Supprimer pr√©fixes techniques
        if new_name.startswith(("api.", "fields.", "properties.")):
            new_name = new_name.split(".", 1)[1]
        
        # Remplacements standards pour lisibilit√©
        replacements = {
            "_": " ",
            "code_dep": "D√©partement",
            "code_departement": "D√©partement", 
            "departement": "D√©partement",
            "dep": "D√©partement",
            "annee": "Ann√©e",
            "year": "Ann√©e",
            "date": "Date",
            "couverture": "Couverture (%)",
            "taux": "Taux (%)",
            "pct": "Pourcentage",
            "y_pred": "Taux pr√©dit (%)",
            "population": "Population",
            "doses": "Doses",
            "actes": "Actes",
            "passages": "Passages",
            "incidence": "Incidence",
            "serie": "Public concern√©",
            "Serie": "Public concern√©",
        }
        
        for old, new in replacements.items():
            if old in new_name.lower():
                new_name = new_name.replace(old, new)
                break
        
        # Capitaliser premi√®re lettre
        new_name = " ".join(word.capitalize() for word in new_name.split())
        rename_map[col] = new_name
    
    return df.rename(columns=rename_map)

def _requests_session():
    s = requests.Session()
    s.headers.update({"User-Agent": "streamlit-odisse/1.0"})
    return s

def _friendly_network_error(e: Exception) -> str:
    msg = str(e)
    tips = []
    if any(k in msg for k in ["Failed to establish a new connection", "Device or resource busy", "Errno 16"]):
        tips.append("Sortie r√©seau bloqu√©e : dans Snowflake, activer une External Access Integration pour "
                    "`odisse.santepubliquefrance.fr:443` et `raw.githubusercontent.com:443` puis l‚Äôattacher √† l‚Äôapp.")
    if "HTTPSConnectionPool" in msg:
        tips.append("V√©rifier DNS/SSL plateforme et l‚ÄôURL.")
    return " ".join(tips) or msg

@st.cache_data(show_spinner=True, ttl=3600)
def fetch_api(url: str) -> pd.DataFrame:
    """T√©l√©charge un endpoint ODISSE (retries + JSON/CSV robustes)."""
    s = _requests_session()
    last_exc = None
    for attempt in range(3):
        try:
            # S‚Äôassure que limit=100 est pr√©sent
            if "limit=" not in url:
                sep = "&" if "?" in url else "?"
                url = f"{url}{sep}limit=100"

            r = s.get(url, timeout=(5, 30))
            r.raise_for_status()
            ct = (r.headers.get("Content-Type") or "").lower()

            if "json" in ct or url.endswith(".json"):
                data = r.json()
                if isinstance(data, dict) and "results" in data:
                    return pd.json_normalize(data["results"])
                if isinstance(data, list):
                    return pd.json_normalize(data)
                return pd.json_normalize(data)

            # fallback CSV si mauvais content-type
            try:
                return pd.read_csv(io.BytesIO(r.content))
            except Exception:
                raise ValueError("R√©ponse non JSON/CSV. L‚ÄôAPI devrait renvoyer du JSON.")
        except Exception as e:
            last_exc = e
            if attempt == 2:
                raise RuntimeError(_friendly_network_error(e)) from e
            time.sleep(1 + attempt)  # backoff
    raise RuntimeError(_friendly_network_error(last_exc or Exception("Erreur inconnue")))

@st.cache_data(show_spinner=True)
def fetch_csv(path: str) -> pd.DataFrame:
    """Charge un CSV local (dans le repo)."""
    if not os.path.exists(path):
        raise FileNotFoundError(f"Fichier introuvable : {path} (mets-le dans le repo)")
    
    # Essayer diff√©rents s√©parateurs et encodages
    for sep in (",", ";"):
        for enc in ("utf-8", "latin-1"):
            try:
                df = pd.read_csv(path, encoding=enc, sep=sep)
                # V√©rifier que le CSV a √©t√© bien pars√© (plus d'une colonne ou colonnes valides)
                if len(df.columns) > 1 or (len(df.columns) == 1 and not df.columns[0].count(";")):
                    return df
            except (UnicodeDecodeError, pd.errors.ParserError):
                continue
    
    # Fallback par d√©faut
    return pd.read_csv(path)

def norm_dep_code(df: pd.DataFrame) -> tuple[pd.DataFrame, str | None]:
    dep_col = None
    lower_map = {c.lower(): c for c in df.columns}
    for cand in DEP_CANDIDATES:
        if cand in df.columns or cand.lower() in lower_map:
            dep_col = cand if cand in df.columns else lower_map[cand.lower()]
            break
    if dep_col is None:
        return df, None
    out = df.copy()
    out.rename(columns={dep_col: "dep"}, inplace=True)
    out["dep"] = out["dep"].astype(str).str.strip()
    out["dep"] = out["dep"].apply(
        lambda x: x if x.upper() in ["2A", "2B"]
        else (x.zfill(2) if x.isdigit() and len(x) <= 2 else x)
    )
    return out, "dep"

def infer_dates(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    # parse ann√©es
    for c in YEAR_CANDIDATES:
        if c in out.columns:
            try:
                out[c] = pd.to_numeric(out[c], errors="coerce").astype("Int64")
            except Exception:
                pass
    # parse dates g√©n√©riques
    for c in DATE_CANDIDATES:
        if c in out.columns:
            try:
                out[c] = pd.to_datetime(out[c], errors="coerce")
            except Exception:
                pass
    # tentative large prudente
    for c in out.columns:
        if out[c].dtype == object:
            try:
                parsed = pd.to_datetime(out[c])  # l√®ve si illisible ‚Üí on ignore
                if pd.api.types.is_datetime64_any_dtype(parsed):
                    out[c] = parsed
            except Exception:
                pass
    return out

def pick_numeric(df: pd.DataFrame) -> list[str]:
    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    pref = [c for c in num_cols if any(k in c.lower()
            for k in ["couverture", "grippe", "covid", "hpv",
                      "meningo", "taux", "ratio", "pourcent", "pct",
                      "value", "nb", "passages", "actes", "incidence", "doses", "y_pred"])]
    return pref if pref else num_cols

def choose_value_col(df: pd.DataFrame, prefs: list[str] | None) -> str | None:
    if prefs:
        for p in prefs:
            for col in df.columns:
                if col.lower() == p.lower() or p.lower() in col.lower():
                    return col
    num_cols = pick_numeric(df)
    return num_cols[0] if num_cols else None

def show_choropleth_dep(df: pd.DataFrame, dep_col: str, val_col: str, title="Carte par d√©partement"):
    try:
        import geopandas as gpd, folium, json as _json
    except Exception:
        st.info("Carte non disponible (geopandas/folium non install√©s).")
        return
    try:
        geo_url = "https://raw.githubusercontent.com/gregoiredavid/france-geojson/master/departements.geojson"
        gdf_geo = gpd.read_file(geo_url)[["code", "geometry"]].rename(columns={"code": "dep"})
        tmp = df[[dep_col, val_col]].dropna().copy()
        tmp.rename(columns={dep_col: "dep", val_col: "value"}, inplace=True)
        tmp["dep"] = tmp["dep"].astype(str).str.strip().apply(
            lambda x: x if x.upper() in ["2A", "2B"]
            else (x.zfill(2) if x.isdigit() and len(x) <= 2 else x)
        )
        merged = gdf_geo.merge(
            tmp.groupby("dep", as_index=False)["value"].mean(), on="dep", how="left"
        )
        m = folium.Map(location=(46.6, 2.5), zoom_start=5, tiles="cartodbpositron")
        folium.Choropleth(
            geo_data=_json.loads(merged.to_json()),
            data=merged,
            columns=["dep", "value"],
            key_on="feature.properties.dep",
            fill_opacity=0.8,
            line_opacity=0.4,
            legend_name=val_col,
        ).add_to(m)
        st.subheader(title)
        st.components.v1.html(m._repr_html_(), height=600, scrolling=False)
    except Exception as e:
        st.error(f"Carte indisponible : {e}")

def show_risk_map_regions(df: pd.DataFrame, region_col: str, risk_col: str, title="Carte de risque grippe par r√©gion"):
    """Affiche une carte avec code couleur : <25 = moins de risque (vert), >25 = plus de risque (rouge)."""
    try:
        import geopandas as gpd, folium, json as _json
        from folium import GeoJson
    except Exception:
        st.info("Carte non disponible (geopandas/folium non install√©s).")
        return
    try:
        # GeoJSON des r√©gions fran√ßaises
        geo_url = "https://raw.githubusercontent.com/gregoiredavid/france-geojson/master/regions.geojson"
        gdf_geo = gpd.read_file(geo_url)[["nom", "geometry"]].rename(columns={"nom": "region"})
        
        tmp = df[[region_col, risk_col]].dropna().copy()
        tmp.rename(columns={region_col: "region", risk_col: "risk_score"}, inplace=True)
        
        # Normaliser les noms de r√©gions
        tmp["region"] = tmp["region"].str.strip()
        gdf_geo["region"] = gdf_geo["region"].str.strip()
        
        merged = gdf_geo.merge(tmp, on="region", how="left")
        
        # Cr√©er la carte
        m = folium.Map(location=(46.6, 2.5), zoom_start=6, tiles="cartodbpositron")
        
        # Fonction de style bas√©e sur le score de risque
        def style_function(feature):
            risk = feature['properties'].get('risk_score')
            if risk is None:
                return {'fillColor': 'gray', 'color': 'black', 'weight': 1, 'fillOpacity': 0.3}
            elif risk < 25:
                # Moins de risque : vert
                return {'fillColor': 'green', 'color': 'black', 'weight': 1, 'fillOpacity': 0.6}
            else:
                # Plus de risque : rouge (plus fonc√© si score √©lev√©)
                intensity = min((risk - 25) / 75, 1)  # normaliser entre 25 et 100
                red_val = int(255)
                green_val = int(255 * (1 - intensity))
                color = f'#{red_val:02x}{green_val:02x}00'
                return {'fillColor': color, 'color': 'black', 'weight': 1, 'fillOpacity': 0.7}
        
        # Ajouter les r√©gions avec style conditionnel
        GeoJson(
            merged,
            style_function=style_function,
            tooltip=folium.GeoJsonTooltip(
                fields=['region', 'risk_score'],
                aliases=['R√©gion:', 'Score de risque:'],
                localize=True
            )
        ).add_to(m)
        
        # L√©gende personnalis√©e
        legend_html = '''
        <div style="position: fixed; bottom: 50px; left: 50px; width: 220px; height: 120px; 
                    background-color: white; border:2px solid grey; z-index:9999; font-size:14px;
                    padding: 10px">
        <p style="margin: 0; font-weight: bold;">Niveau de risque grippe</p>
        <p style="margin: 5px 0;"><span style="background-color: green; padding: 3px 10px; color: white;">‚ñ†</span> Faible risque (score < 25)</p>
        <p style="margin: 5px 0;"><span style="background-color: orange; padding: 3px 10px; color: white;">‚ñ†</span> Risque moyen (25-50)</p>
        <p style="margin: 5px 0;"><span style="background-color: red; padding: 3px 10px; color: white;">‚ñ†</span> Risque √©lev√© (> 50)</p>
        </div>
        '''
        m.get_root().html.add_child(folium.Element(legend_html))
        
        st.subheader(title)
        st.components.v1.html(m._repr_html_(), height=600, scrolling=False)
    except Exception as e:
        st.error(f"Carte de risque indisponible : {e}")

def show_time_series(df: pd.DataFrame, title="S√©ries temporelles (auto)"):
    if px is None:
        st.info("Plotly non disponible, s√©ries temporelles d√©sactiv√©es.")
        return
    # choisir une colonne temporelle
    time_col = None
    for c in DATE_CANDIDATES + YEAR_CANDIDATES:
        if c in df.columns:
            time_col = c
            break
    if time_col is None:
        return
    local = df.dropna(subset=[time_col]).copy()
    if not pd.api.types.is_datetime64_any_dtype(local[time_col]):
        if pd.api.types.is_integer_dtype(local[time_col]) or local[time_col].astype(str).str.fullmatch(r"\d{4}").any():
            try:
                local[time_col] = pd.to_datetime(local[time_col].astype(str) + "-01-01", errors="coerce")
            except Exception:
                pass
        else:
            try:
                local[time_col] = pd.to_datetime(local[time_col], errors="coerce")
            except Exception:
                pass
    local = local.dropna(subset=[time_col]).sort_values(time_col)
    if local.empty:
        return
    y_cols = pick_numeric(local)[:3]
    if not y_cols:
        return
    fig = px.line(local, x=time_col, y=y_cols, title=title)
    st.plotly_chart(fig, use_container_width=True)

def get_unit_label(col_name: str) -> str:
    """D√©termine l'unit√© d'une colonne selon son nom."""
    col_lower = col_name.lower()
    if any(term in col_lower for term in ["taux", "couverture", "pct", "pourcent", "%"]):
        return "%"
    elif any(term in col_lower for term in ["population", "hab"]):
        return "habitants"
    elif any(term in col_lower for term in ["doses", "actes", "passages", "nombre", "nb"]):
        return "nombre"
    elif "incidence" in col_lower:
        return "pour 100k hab."
    else:
        return ""

def safe_scatter(df: pd.DataFrame, title: str):
    if px is None:
        return
    num_cols = pick_numeric(df)
    if len(num_cols) < 2:
        return
    x, y = num_cols[0], num_cols[1]
    tmp = df[[x, y]].dropna()
    if tmp.empty:
        return
    
    # Construire un titre explicite avec les noms de variables et unit√©s
    x_unit = get_unit_label(x)
    y_unit = get_unit_label(y)
    x_label = f"{x} ({x_unit})" if x_unit else x
    y_label = f"{y} ({y_unit})" if y_unit else y
    scatter_title = f"Relation entre {x} et {y}"
    
    # trendline seulement si statsmodels est dispo
    try:
        import statsmodels.api as sm  # noqa: F401
        trend = "ols"
    except Exception:
        trend = None
    
    fig = px.scatter(tmp, x=x, y=y, trendline=trend, title=scatter_title,
                     labels={x: x_label, y: y_label})
    st.plotly_chart(fig, use_container_width=True)

# ===== 3) Rendu sp√©cifique PR√âDICTION =======================================
def normalize_prediction_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Harmonise quelques colonnes usuelles du CSV de pr√©diction."""
    out = df.copy()
    # Uniformise quelques noms (sans casser si absents)
    rename_map = {}
    for c in out.columns:
        lc = c.strip().lower()
        if lc == "serie":
            rename_map[c] = "Serie"
        elif lc in ("annee", "ann√©e", "annee calcul√©e", "ann√©e calcul√©e"):
            rename_map[c] = "annee"
        elif lc in ("y_pred", "yÃÇ", "yhat", "y_hat", "prediction", "pr√©diction"):
            rename_map[c] = "y_pred"
        elif lc in ("population", "pop"):
            rename_map[c] = "population"
        elif "doses totales" in lc:
            rename_map[c] = "Doses totales arrondies"
        elif "marges" in lc:
            rename_map[c] = "Doses avec marges arrondies"
        elif "dose par sch√©ma" in lc or "doses par schema" in lc:
            rename_map[c] = "dose par sch√©ma"
    if rename_map:
        out = out.rename(columns=rename_map)

    # Types
    if "annee" in out.columns:
        try:
            out["annee"] = pd.to_numeric(out["annee"], errors="coerce").astype("Int64")
        except Exception:
            pass
    for col in ["y_pred", "population", "Doses totales arrondies", "Doses avec marges arrondies"]:
        if col in out.columns:
            out[col] = pd.to_numeric(out[col], errors="coerce")

    return out

def render_prediction_panel(df: pd.DataFrame, label: str, is_risk_map: bool = False):
    st.markdown("**üü£ Pr√©diction pour l'ann√©e en cours**")
    df = normalize_prediction_columns(df)

    # Filtre ann√©e si pr√©sence de plusieurs ann√©es
    if "annee" in df.columns and df["annee"].notna().any():
        years = sorted({int(x) for x in df["annee"].dropna().unique()})
        year_sel = st.selectbox("Ann√©e", options=years, index=len(years)-1 if years else 0)
        df = df[df["annee"] == year_sel]

    # Nettoyer les noms de colonnes et masquer les colonnes vides
    df_display = clean_column_names(df)
    # Supprimer colonnes compl√®tement vides
    df_display = df_display.dropna(axis=1, how='all')
    
    # Afficher l√©gende si termes m√©dicaux d√©tect√©s
    legends = get_legend_for_data(df)
    if legends:
        with st.expander("‚ÑπÔ∏è L√©gende et d√©finitions", expanded=False):
            for leg in legends:
                st.markdown(f"- {leg}")
    
    # Formater les colonnes num√©riques pour un meilleur affichage
    df_display_formatted = df_display.copy()
    for col in df_display_formatted.columns:
        if pd.api.types.is_numeric_dtype(df_display_formatted[col]):
            # Arrondir √† 2 d√©cimales pour les nombres
            df_display_formatted[col] = df_display_formatted[col].round(2)
    
    st.dataframe(
        df_display_formatted, 
        use_container_width=True,
        hide_index=True,
        column_config={
            col: st.column_config.NumberColumn(
                col,
                format="%.2f" if pd.api.types.is_float_dtype(df_display_formatted[col]) else "%d"
            ) for col in df_display_formatted.columns if pd.api.types.is_numeric_dtype(df_display_formatted[col])
        }
    )

    # Si carte de risque grippe par r√©gions
    if is_risk_map:
        # Chercher colonne r√©gion et score de risque
        region_col = None
        risk_col = None
        
        for col in df.columns:
            col_lower = col.lower()
            if "region" in col_lower or col == "Region":
                region_col = col
            if "risque" in col_lower or "risk" in col_lower or "score" in col_lower:
                risk_col = col
        
        if region_col and risk_col:
            # Convertir le score en num√©rique
            df[risk_col] = pd.to_numeric(df[risk_col], errors='coerce')
            
            show_risk_map_regions(df, region_col, risk_col, "Pr√©diction risque grippe 2025 par r√©gion")
            
            # KPIs sp√©cifiques
            st.markdown("#### üìä Analyse du risque")
            cols = st.columns(3, gap="large")
            risk_values = df[risk_col].dropna()
            if not risk_values.empty:
                cols[0].metric("Score moyen", f"{risk_values.mean():.1f}")
                cols[1].metric("R√©gions √† risque √©lev√© (>50)", len(risk_values[risk_values > 50]))
                cols[2].metric("R√©gions √† faible risque (<25)", len(risk_values[risk_values < 25]))
        return

    # KPIs si colonnes pr√©sentes (pour autres pr√©dictions)
    cols = st.columns(3, gap="large")
    if "y_pred" in df.columns:
        cols[0].metric("Taux pr√©dit (moy.)", f"{df['y_pred'].mean():.2f}")
    if "Doses totales arrondies" in df.columns:
        cols[1].metric("Doses totales (arr.)", f"{int(round(df['Doses totales arrondies'].sum())):,}".replace(",", " "))
    if "Doses avec marges arrondies" in df.columns:
        cols[2].metric("Doses + marge 5% (arr.)", f"{int(round(df['Doses avec marges arrondies'].sum())):,}".replace(",", " "))

    # Graph barres par Serie (public concern√©)
    if px is not None and "Serie" in df.columns:
        ycol = "y_pred" if "y_pred" in df.columns else choose_value_col(df, ["taux", "couverture"])
        if ycol:
            fig = px.bar(df.dropna(subset=[ycol]), x="Serie", y=ycol, title="Taux de vaccination par public (%)")
            st.plotly_chart(fig, use_container_width=True)

# ===== 4) Chargement & rendu g√©n√©rique =======================================
def load_dataset(ds: dict) -> pd.DataFrame:
    if ds["kind"] == "api":
        return fetch_api(ds["path"])
    elif ds["kind"] == "csv":
        return fetch_csv(ds["path"])
    else:
        raise ValueError(f"Type de dataset inconnu: {ds['kind']}")

def render_dataset_panel(ds: dict):
    label = ds["label"]
    with st.spinner(f"Chargement ‚Äî {label}"):
        try:
            raw = load_dataset(ds)
        except Exception as e:
            st.error(f"Erreur de chargement pour ¬´ {label} ¬ª : {e}")
            return

    # Si dataset de pr√©diction : UI d√©di√©e, pas de carte
    if ds.get("is_prediction"):
        render_prediction_panel(raw, label, is_risk_map=ds.get("is_risk_map", False))
        return

    # Sinon : pipeline normal
    df, dep_col = norm_dep_code(raw)
    df = infer_dates(df)

    # Nettoyer les noms de colonnes et masquer les colonnes vides
    df_display = clean_column_names(df)
    # Supprimer colonnes compl√®tement vides
    df_display = df_display.dropna(axis=1, how='all')
    
    # Afficher l√©gende si termes m√©dicaux d√©tect√©s
    legends = get_legend_for_data(df)
    if legends:
        with st.expander("‚ÑπÔ∏è L√©gende et d√©finitions", expanded=False):
            for leg in legends:
                st.markdown(f"- {leg}")
    
    # Formater les colonnes num√©riques pour un meilleur affichage
    df_display_formatted = df_display.head(50).copy()
    for col in df_display_formatted.columns:
        if pd.api.types.is_numeric_dtype(df_display_formatted[col]):
            # Arrondir √† 2 d√©cimales pour les nombres
            df_display_formatted[col] = df_display_formatted[col].round(2)
    
    st.dataframe(
        df_display_formatted, 
        use_container_width=True,
        hide_index=True,
        column_config={
            col: st.column_config.NumberColumn(
                col,
                format="%.2f" if pd.api.types.is_float_dtype(df_display_formatted[col]) else "%d"
            ) for col in df_display_formatted.columns if pd.api.types.is_numeric_dtype(df_display_formatted[col])
        }
    )

    # Carte uniquement si on a une colonne d√©partement
    val_col = choose_value_col(df, ds.get("value_pref"))
    if dep_col and val_col:
        show_choropleth_dep(df, dep_col, val_col, f"{label} ‚Äî {val_col}")
    else:
        st.info("Pas de couple (d√©partement, valeur) d√©tect√© pour la carte.")

    # S√©ries & Corr√©lation (si donn√©es temporelles et num√©riques)
    show_time_series(df, "√âvolution temporelle (auto)")
    safe_scatter(df, "Analyse de corr√©lation")

# ===== 5) UI ‚Äî s√©lection & rendu =============================================
st.markdown("### üó∫Ô∏è S√©lectionne les datasets √† afficher")
options = [ds["label"] for ds in DATASETS]
default_sel = [ds["label"] for ds in DATASETS]  # tout coch√© par d√©faut
choices = st.multiselect("Datasets", options=options, default=default_sel)

tabs = st.tabs(choices if choices else ["Aucun dataset"])
if choices:
    for tab, label in zip(tabs, choices):
        with tab:
            ds = next(d for d in DATASETS if d["label"] == label)
            render_dataset_panel(ds)
else:
    with tabs[0]:
        st.info("S√©lectionne au moins un dataset pour afficher les visuels.")


